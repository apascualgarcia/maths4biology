
% \setcounter{page}{12}

\section{Limits and Derivatives}\label{diff}


\subsection{Limits}

Many times we are interested in studying the behaviour of a function in the proximity
of a certain value. This value can be, in principle, any value, 
but the use of limits typically concerns those values that are singular, perhaps because do not belong to the domain of the function or because we are interested
in its behaviour at the infinity. A typical question of interest in biology, may be
which is the expected behaviour of a population if we assume that its size is so large
that we consider it infinite. In that case, we will take the limit of the function
describing the population at the infinite, and this is many times useful because
the function may be simplified, and will allow us to perform further analytical development.

As we said, the limit of a function at an arbitrary point may be easy to compute. To compute
the limit of a function $f(x)$ as $x$ approaches $a$, that we write $\lim_{x\rightarrow a}f(x)$,
we start evaluating $f(a)$.

{\bf Example:} 

\bnn
 \lim_{x\rightarrow 1}f(x)= \lim_{x\rightarrow 1} \frac{1}{2}(x+3) \underbrace{=}_{f(1)} \frac{1}{2}(1+3) = 2.
\enn

In this case, it was very easy and not very interesting. Let's see a precise 
definition of limit which is, perhaps, the most abstract definition we will learn in this course, and how we can solve more complicated examples. Learning the definition will be a good training for our mind to get into the abstraction of concepts such as the infinity, and to open
the door towards the important world of the derivatives. 

{\bf Definition:} The limit of a function $f(x)$ as $x$ approaches $a$, that we write $\lim_{x\rightarrow a}f(x)$,
is a number $l$ such that, given any target distance $\varepsilon$ between $f(x)$ and $l$, it is always possible to find a value $x$ such that its distance $\delta$ with respect to $a$ is such that the distance between $f(x)$ and $l$ remain lower than $\varepsilon$, i.e.

\bnn
\lim_{x\rightarrow a}f(x)\Leftrightarrow\forall\varepsilon>0\ \exists\delta>0\ /\ 0<|x-a|<\delta\Rightarrow |f(x)-l|<\varepsilon
\enn
\vs

Wow, the definition is really ugly. Let's try to solve the above example with this definition.

{\bf Example:} Demonstrate that the $\lim_{x\rightarrow 1} \frac{1}{2}(x+3)=2$ using the definition of limit.

What we look for is a positive distance $\delta$ such that if we fix an arbitrary $\varepsilon$, if $|x-1|<\delta$ then $ |\frac{1}{2}(x+3)-2|<\varepsilon$. We can rewrite the latter condition:

\bnn
 |\frac{x+3-4}{2}|<\varepsilon \Rightarrow |\frac{x-1}{2}| < \varepsilon \Rightarrow |x-1|<2\varepsilon.
\enn

And it looks like it is easy to find $\delta$, because if we make $\delta = 2\varepsilon$, it actually happens that:

\bnn
0<|x-1|<2\varepsilon \Rightarrow |\frac{1}{2}(x+3)-2|<\varepsilon,
\enn

which fulfills the definition of limit. Let's now look for a more interesting example, because
we look for a limit at a pathological value.

{\bf Example:} Demonstrate that the $\lim_{x\rightarrow 0} x\sin(1/x)=0$ using the definition of limit.

Again, we look for a positive distance $\delta$ such that if $|x-0|<\delta$ then 
$x\sin(1/x)-0 < \varepsilon$. Given that the image of the $\sin$ is bounded between
zero and one, we note that $0 \leq \sin(1/x)\leq 1$ and, hence, $|x\sin(1/x)|\leq |x| \leq \varepsilon$. Therefore, it is true that this is the limit, because it is enough to say that
$\delta = \varepsilon$ to see that if $|x| < \varepsilon$ also $|x\sin(1/x)|<\varepsilon$,
which is what we were willing to demonstrate.

\subsection{Lateral limits, continuity of functions and assymptotes}

The lateral limit is the limit of a function when we approximate a value $a$ either from $x$ values smaller than $a$ (it is said \emph{from the left} and we write $\lim_{x\rightarrow a^{-}}f(x)$) or from $x$ values larger than $a$ (\emph{from the right}, $\lim_{x\rightarrow a^{+}}f(x)$). The
formal definitions are:

\bnn
\lim_{x\rightarrow a^{-}}f(x)\Leftrightarrow\forall\varepsilon>0\ \exists\delta>0\ /\ 0<x-a<\delta\Rightarrow |f(x)-l|<\varepsilon \\
\lim_{x\rightarrow a^{+}}f(x)\Leftrightarrow\forall\varepsilon>0\ \exists\delta>0\ /\ 0<a-x<\delta\Rightarrow |f(x)-l|<\varepsilon.
\enn
\vs

The definition of lateral limits leads to two important theorems:

{\bf Theorem: } The limit of a function exists if and only if its lateral limits exist and are equal.

{\bf Theorem: } A function is {\em continuous} in $x_0$ if the limit exist and it is equal
to the value of the function at $x_0$.

With these theorems we can determine if a function is continuous (we will investigate its pathological values) and, if it is not continuous, which kind of discontinuity it has.

{\bf Example: } Continuous function.

\bnn 
  f(x)=\begin{cases}
    x+3 & \text{if $x <1$}.\\
    4 & \text{if $x>1$}.
  \end{cases}
\enn

In this case, we observe a possible pathological value at $x=1$. Nevertheless, the function
is continuous because $\lim_{x\rightarrow 1^{+}}f(x) =\lim_{x\rightarrow 1^{-}}f(x) =f(1)$.

{\bf Example: } Function with a "removable" discontinuity.

\bnn 
  g(x)=\begin{cases}
    3 & \text{if $x \neq 0$}.\\
    2 & \text{if $x = 0$}.
  \end{cases}
\enn

With this function it happens that, at $x=0$, the lateral limits exists and are equal, but
the function takes a different value, i.e. 
$\lim_{x\rightarrow 0^{+}}g(x) =\lim_{x\rightarrow 0^{-}}g(x) \neq g(0)$. We say
that the function is discontinuous but, since the limit exists and it  is finite, we call
it a "removable" discontinuity (in some sense we think of the limit as the "true" value).

{\bf Example: } Function with a "finite jump" discontinuity.

\bnn 
  h(x)=\begin{cases}
    3 & \text{if $x < 0$}.\\
    4 & \text{if $x > 0$}.\\
    2 & \text{if $x = 0$}.
  \end{cases}
\enn

This function it happens that, at $x=0$, the lateral limits exists but are not equal, but
there is a finite difference between both values so we say that it is a finite discontinuity.

{\bf Example: } Function with an infinite discontinuity.

\bnn
i(x)=\frac{x+1}{x-2}\Rightarrow \begin{cases}
\lim_{x\rightarrow 2^{-}} i(x) = -\infty \\
\lim_{x\rightarrow 2^{+}} i(x) = \infty 
\end{cases}
\enn

{\bf Definition:} We will say that a function $fa(x)$ approaches assymptotically to a line (e.g. $y=a$ or $x=a$), or that the line is an assymptote of the function, if the distance between the function and
the curve approaches to zero when one or both $x$ and $f(x)$ tend to infinity.

{\bf Examples:}
\begin{enumerate}
	\item Vertical assymptote: $\lim_{x\rightarrow a} f(x) = \infty$
	\item Horizontal assymptote: $\lim_{x\rightarrow \infty} f(x) = a$
\end{enumerate}

\subsection{Limits with an indeterminate form}

In many situations, when we evaluate a limit we do not have enough information to
determine if the limit exists, in which case we face an {\em indeterminate form}. These
forms are functions that, after being evaluated, lead to expressions of this kind:

\bnn
  	\frac{0}{0}, \frac{\infty}{\infty}, \infty-\infty, 1^\infty, 0^0, 0Â·\infty, \infty^0, 
\enn

and require special techniques to solve them. In the following subsections we summarize
some of the most common techniques.

\subsubsection{Rational limits to infinity}

For rational functions, we should consider how fast the functions in the numerator
and in the denominator tend to infinity. There is an order on how fast functions tend
to infinity:

\bnn
x^{kx}\  > b^x > x^m > \log x
\enn

Where the symbol $>$ means that one function grows faster than the other one, and $k>0$.

{\bf Examples:}
\begin{enumerate}
\item $\lim_{x\rightarrow \infty} \frac{e^{3x}}{4x^2}=\infty$
\item $\lim_{x\rightarrow \infty} \frac{\log(x)}{x}=0$
\item $\lim_{x\rightarrow \infty} \frac{18x^2+1}{32x^2+3}=\frac{9}{16}$
\end{enumerate}

\subsubsection{Infinitesimal equivalents}

On the other hand, when two functions become infinitesimally small when they
converge towards the same point, we will say that they are {\em infinitesimal equivalents},
and we can use this fact to find limits of rational functions. We say that $f(x)$ and 
$g(x)$ are infinitesimal equivalents around $a$ if

\bnn
	\lim_{x\rightarrow a}\frac{f(x)}{g(x)}.	
\enn

Some common infinitesimal equivalents are:
\begin{enumerate}
 \item When $x \rightarrow 0$:
   \bnn
      x \simeq \sin(x); \mbox{  } x \simeq \tan(x); \mbox{  } x \simeq \log(1+x); \mbox{  } x \simeq e^x-1 \\
      x \simeq \arcsin(x); \mbox{  } x \simeq \arctan(x); \mbox{  } 1-\cos(x) \simeq \frac{x^2}{2}
   \enn
    \item When $x \rightarrow 1$:
   \bnn
      x -1\simeq \log(x);
   \enn
\end{enumerate}

{\bf Example:}
\bnn
 	\lim_{x\rightarrow 0}\frac{\sin(x)}{\ln(1+x)} = 	\lim_{x\rightarrow 0}\frac{x}{x} =1
\enn

{\bf Example:}
\bnn
 	\lim_{x\rightarrow 0}\frac{\tan(x)(1-\cos(x)}{x^3} = 	\lim_{x\rightarrow 0}\frac{x(1-\cos(x))}{x^3} = \lim_{x\rightarrow 0}\frac{x^2/2}{x^2} = \frac{1}{2}.
\enn

\subsubsection{Algebraic operations}

Many times, we can simplify the expression or find an appropriate change of variables
before we compute the limit, that will solve the indeterminacy.

{\bf Example:} Rational factorization

\bnn
\lim_{x\rightarrow 3}\frac{x^2-9}{x^2-5x+6} = \lim_{x\rightarrow 3}\frac{(x+3)(x-3)}{(x-2)(x-3)}=\lim_{x\rightarrow 3}\frac{x+3}{x-2}=6.
\enn

{\bf Example:} Rational factorization

\bnn
\lim_{x\rightarrow 3}\frac{\sqrt{x+1}-2}{x-3} = \lim_{x\rightarrow 3}\frac{(\sqrt{x+1}-2)(\sqrt{x+1}+2)}{(x-3)(\sqrt{x+1}+2)}= \\
=\lim_{x\rightarrow 3}\frac{(x+1)-4}{(x-3)(\sqrt{x+1}+2)}=\lim_{x\rightarrow 3}\frac{1}{\sqrt{x+1}+2}=\frac{1}{4}.
\enn

{\bf Example:} Change of variables

\bnn 
 \begin{array}{rcl}
      \lim_{x\rightarrow 2}\frac{\sin(x^3-8)}{x-2}& \underbrace{=}_{\uparrow} \lim_{t\rightarrow 0}\frac{\sin(t+2)^3-8}{t}\\
	                                \mbox{change vars:}& \begin{cases} t=x-2 \Rightarrow x=t+2 \\ 
	                                                                   x \rightarrow 2 \Rightarrow t\rightarrow 0
	\end{cases}
	\end{array}
\enn

The change of variables does not seem to help much, but if we remind the formula for the cube
of a binomial, which we recall here:

\bnn
 (a+b)^3 = a^3 + b^3 + 3a^2b + 3ab^2 \\
 (a-b)^3 = a^3 - b^3 - 3a^2b + 3b^2a,
\enn

and we apply it to $(t+2)^3=t^3+6t^2+12t+8$, the numerator simplifies:

\bnn
 \begin{array}{rcl}
 \lim_{t\rightarrow 0}\frac{\sin(t^3+6t^2+12t)}{t}& \underbrace{=}_{\uparrow} \lim_{t\rightarrow 0}\frac{t(t^2+6t+12)}{t}=12.\\
 \mbox{Infinitesimal equivalents:}& \sin(f(x)) \simeq f(x)
 \end{array}
\enn


\subsubsection{L'H{\^o}pital rule}

For indeterminate forms of the type $0/0$ or $\infty/\infty$ there is a rule that may
allow us to find a limit. But we need to learn first derivatives! We will come back to this
question in the section of Applications of Derivatives.

\subsection{Derivatives: the Difference Quotient}
First derivatives of simple functions were studied by Galileo
Galilei (1564-1642) and Johannes Kepler (1571-1630). A systematic
theory of differential calculus was developed by Isaac Newton
(1643-1727) and Gottfried Wilhelm Leibniz (1646-1710).

The difference quotient becomes the differential in the limit
$h\rightarrow 0$ and describes the slope of a function $y=f(x)$
at a given point $x$.
\bnn y'(x)=\frac{dy}{dx}=\underset{h\rightarrow0}{\lim} \frac{y(x+h) - y(x)}{h} \enn

\begin{figure}[!h]
    \centerline{\epsfxsize=10cm \epsfysize=9cm \epsfbox{matlab/fig18.eps}} \svs
    \caption{The slope of a curve is found from its derivative.} \label{fig20}
\end{figure} \vs

\begin{figure}[!h]
    \centerline{\epsfxsize=10cm \epsfbox{matlab/fig19.eps}} \svs
    \caption{Slope as h$\rightarrow 0$.} \label{fig21}
\end{figure} \svs

{\bf Notation:} The limit value of the difference quotient is called the
derivative of a function $f(x)$. Derivatives are denoted by
\bnn y'(x)\;,\;\frac{dy}{dx}\;,\;\frac{df}{dx}\;,\;\frac{d}{dx}f(x) \qquad
\mbox{or sometimes in physics:} \;\; \dot{y}(t) \enn

{\bf Note:} Here we consider first-order derivatives only.

\vs
{\bf Example:}  $\qquad y = f(x) =x^2$
\bnn
y'\,=\,\frac{dy}{dx}=\underset{h\rightarrow 0}{\lim} \frac{(x+h)^2-x^2}{h}
\,=\,\underset{h\rightarrow 0}{\lim} \frac{x^2 +2hx +h^2 -x^2}{h}
\,=\,\underset{h\rightarrow 0}{\lim} \frac{2hx+h^2}{h} = 2x
\enn \vs

\subsection{Derivatives of Elementary Functions}

\subsubsection{Polynomials}
\vspace*{-2mm}\bnn y=x^2 \quad \rightarrow \quad \frac{dy}{dx}=2x
\qquad \qquad \qquad \mbox{more general:} \quad
y=x^n \quad \rightarrow \quad \frac{dy}{dx}=nx^{n-1} \enn

\subsubsection{Trigonometric functions}
\vspace*{-2mm}\bnn
y=\sin x \quad \rightarrow \quad \frac{dy}{dx}=\cos x \qquad \qquad \qquad
y=\cos x \quad \rightarrow \qquad \frac{dy}{dx}=-\sin x
\enn

\subsubsection{Exponential functions}
\vspace*{-2mm}\bnn y=e^x \quad \rightarrow \quad \frac{dy}{dx}=e^x \enn

\subsubsection{Hyperbolic functions}
\vspace*{-2mm}\bnn
y=\sinh x \quad \rightarrow \quad \frac{dy}{dx}=\cosh x \qquad \qquad \qquad
y=\cosh x \quad \rightarrow \quad \frac{dy}{dx}=\sinh x
\enn

\subsubsection{Logarithms}
\vspace*{-2mm}\bnn y=\ln x \quad \rightarrow \quad \frac{dy}{dx}=\frac{1}{x} \enn

\subsection{The Basic Rules for Calculating Derivatives}
If the derivatives of two functions $u(x)$ and $v(x)$ exist on an interval
$a<x<b$, then the derivatives of their combinations exist as well, i.e.
\bnn
u+v, \quad \alpha \, u \;\; \mbox{with} \;\; \alpha \in \mathbb{R}, \quad
u\,v, \quad \frac{u}{v} \;\; \mbox{if} \;\; v(x)\not=0 \;\; \mbox{for} \;\; a<x<b
\enn
{\bf Rules:}
\bnn \begin{array}{cc} \svs
\qquad (u + v)'=\frac{d}{dx}\{u+v\}=u' + v' & \qquad\qquad \mbox{\bf derivatives are additive} \qquad\qquad \\ \svs
\qquad (\alpha \, u)'=\frac{d}{dx}\{\alpha \, u\}=\alpha \, u' & 
\qquad\qquad \mbox{\bf multiplication with a scalar} \qquad\qquad \\ \svs
\qquad (u\,v)'=\frac{d}{dx}\{u\,v\}= u'\,v+u\,v'  & \qquad\qquad \mbox{\bf product rule} \qquad\qquad \\ \svs
\qquad (\frac{u}{v})'=\frac{d}{dx}\{\frac{u}{v}\}=\frac{u'\,v - u\,v'}{v^2} & \qquad\qquad \mbox{\bf quotient rule} \qquad\qquad
\end{array} \enn

{\bf Examples:}
\bnn \frac{d}{dx}\{x^{17}+\cos x\} = 17x^{16}-\sin x \enn
\bnn \frac{d}{dx}\{35 \cosh x\}=35 \frac{d}{dx}\,\cosh x=35 \sinh x \enn
\bnn \frac{d}{dx}\{\cos x e^x\} = - \sin x e^x +\cos x e^x = e^x (\cos x - \sin x) \enn
\bnn \frac{d}{dx}\{\frac{\cos x}{e^x}\} = \frac{-\sin x \, e^x - \cos x \, e^x}{e^{2x}}
    =\frac{-e^x \, (\sin x +\cos x)}{e^{2x}}= -\frac{\sin x + \cos x}{e^x} \enn \svs


\subsection{The Chain Rule}
If $u(x)$ and $v(x)$ have derivatives and the image of $v(x)$ is part of the source set of $u(x)$, 
then $u(v(x))$ has a derivative. 

To understand what this complicated sentence means, consider 
$\ln(\cos x)$. Here $u(x)=\ln x$ and $v(x)=\cos x$. The source set of $\cos x$ are all real numbers
$[-\infty, \infty]$, the image set of the cosine are the numbers in the interval [-1, 1], and the source 
set of the logarithm are all positive real numbers $]0, \infty]$. Therefore the image set of the cosine 
and the source set of the logarithm overlap in the interval $]0, 1]$. The source set of $\cos x$ that corresponds
to the image set $]0, 1]$ is given by all numbers where $\cos x$ is positive, i.e. $]-\frac{\pi}{2}, -\frac{\pi}{2}[$,
$]\frac{3\,\pi}{2}, -\frac{5\,\pi}{2}[$, etc., and the function $\ln(\cos x)$ exists and has a derivative for 
these values of $x$.
\bnn [u(v(x))]'=\frac{d}{dx}\{u(v(x))\}= \frac{d\,u(v)}{dv}\;\frac{d\,v(x)}{dx} \qquad\qquad \mbox{\bf chain rule} \enn

{\bf Examples:}
\bnn f(x)=\cos(\alpha x) \quad \rightarrow \quad u(v)=\cos v \quad \mbox{and} \quad v(x)=\alpha x \enn
\bnn \frac{d}{dx}\,\cos \alpha x=\frac{d\,\cos \alpha x}{d\,\alpha x}\;\frac{d\,\alpha x}{dx}
   =(-\sin \alpha x)\,\alpha = -\alpha \sin \alpha x \enn

\bnn f(x)=(2x+5)^3\quad \rightarrow \quad u(v)=v^3 \quad \mbox{and} \quad v(x)=2x+5 \enn
\bnn \frac{d}{dx}\,(2x+5)^3=\frac{d\,(2x+5)^3}{d\,(2x+5)}\,\frac{d\,(2x+5)}{dx}=3\,(2x+5)^2\;2=6\,(2x+5)^2 \enn

\subsection{Selected problems (the page from hell):}
{\bf Important note: Now we can take the derivative of ANY analytic function !!!}

\bnn f(x)=e^{\ln x} \quad \rightarrow \quad u(v)=e^v \quad \mbox{and} \quad v(x)=\ln x \enn
\bnn f'(x)=e^{\ln x}\,\frac{1}{x}=x\,\frac{1}{x}=1 \qquad \mbox{of course we started with}
  \quad f(x)=x \;\; \rightarrow \;\; f'(x)=1
\enn \svs

\bnn f(x)=\sqrt{\sin(3\,\alpha^2\,x^5)} = [\sin(3\,\alpha^2\,x^5)]^\frac{1}{2}=u(v(w(x))) \\
   \hspace*{3cm} \rightarrow \quad u(v)=v^\frac{1}{2} \quad v(w)=\sin(3\,\alpha^2\,x^5)
   \quad w(x)=3\,\alpha^2\,x^5 \enn
\bnn f'(x)=\frac{d\,u(v(w(x)))}{dv}\,\frac{d\,v(w(x))}{dw}\,\frac{d\,w(x)}{dx}
   =\frac{1}{2}\,[\sin(3\,\alpha^2\,x^5)]^{\frac{1}{2}-1} \; \cos(3\,\alpha^2\,x^5) \;
      3\,\alpha\,5\,x^{5-1} \\
   \hspace*{3cm} = \frac{15\,\alpha\,x^4\,\cos(3\alpha^2x^5)}{2\sqrt{\sin(3\alpha^2x^5)}}
      \qquad \mbox{who guessed this result ???}
\enn \svs

\bnn
 f(x)=\frac{3x^2+\cos kx}{\cosh x} \quad \rightarrow \quad
 f'(x)=\frac{(6x-k\sin kx)\cosh x + (3x^2+\cos kx)\sinh x}{\cosh^2x}
\enn

\hspace*{4cm}Also quite ugly, but technically correct !!! \vs

\bnn
f(x)=\cos^2 kx = \cos kx \, \cos kx \quad \rightarrow \quad f'(x)=2\,\cos kx (-\sin kx)\, k
   =-2k\,\cos kx \sin kx \\
   \hspace*{2cm} \mbox{or} \quad \rightarrow \quad (-\sin kx)\,k\,\cos kx+\cos kx (-\sin kx)\,k
       = -2k\,\cos kx \sin kx
\enn \svs

\bnn
f(x)=y=(x^5+e^{\cos kx})^{1/2} \quad \rightarrow \quad
  y'=\frac{1}{2}(x^5+e^{\cos kx})^{-1/2}\,(5\,x^4+e^{\cos kx}(-k\,\sin kx)) \\
    \hspace*{8cm} = \frac{5\,x^4-k\,\sin 2kx \, e^{\cos kx}}{2\,(x^5+e^{\cos kx})^{1/2}}
\enn \svs

\bnn
y=x^x=e^{x\ln x} \quad (\mbox{remember:} \; a^x=e^{x\ln a}) \quad \rightarrow \quad
y'=e^{x\ln x} \, (\ln x + 1) = x^x (\ln x + 1)
\enn \vs


\subsection{Applications of derivatives}

\subsubsection{Maxima, minima, inflection points and convexity}

One of the most useful applications of derivatives is that they allow us
to find properties of functions such as maxima, minima and inflection points. Given a differentiable
function $f$, the procedure to find these points would be:

\begin{enumerate}
	\item Find the first derivative $f'(x)$.
	\item Find the second derivative $f''(x)$
	\item Solve $f'(x)=0$ and find $X=\{x_1,x_2,\ldots,x_N / f'(x)=0,\ \forall i=1,\ldots,N\}$
	\item Substitute the values of $X$ into $f''(x)$ and:
	\begin{enumerate}
		 \item[*] if $f''(x_i)<0$ we have a maxima at $x_i$.
    	 \item[*] if $f''(x_i)<0$ we have a minima at $x_i$.
    	 \item[*] if $f''(x_i)=0$ continue computing derivatives unitil $f^{n)}(x_i)\neq 0$.
    	 \begin{enumerate}
    	 	\item If $n$ is even and $f^{n)}(x_i)< 0$ we have a {\em local} maxima, and if
    	 	$f^{n)}(x_i)> 0$ we have a {\em local} minima .
    	 	\item If  $n$ is odd we get an inflection point.
    	 \end{enumerate}
	\end{enumerate}
	\item Solve  $f''(x)=0$ and find $X=\{x_1,x_2,\ldots,x_N / f''(x)=0,\ \forall i=1,\ldots,N\}$. This is a necessary condition for being an inflection point. Sufficient condition will be that $f^{3)}(x)\neq 0$.
\end{enumerate}

{\bf Note: } These are local minima and maxima at $x_0$, to be global we further require that:
\bnn
	f(x)\leq f(x_0), \quad \forall x \mbox{ (maxima)}\\
		f(x)\geq f(x_0), \quad \forall x \mbox{ (minima)}	
\enn

Similarly, we can find conditions for the concavity of functions. Given a function $f$ and an interval $(a,b)$, we say that the function is concave (up or down) in the interval if:

\bnn
	f'(a) \leq f'(b) \Rightarrow f''(x) > 0 \mbox{, (concave up)} \\
		f'(a) \geq f'(b) \Rightarrow f''(x) < 0 \mbox{, (concave down)}
\enn

{\bf Example:} Find and analyze the critical points of the function$f(x)=x^3-12x^2+45x-30$.

First, we solve $f'(x)=0$ and $f''(x)=0$:

\bnn
f'(x)=3x^2-24x+45=3(x^2-8x+15)=3(x-3)(x-5)=0 \quad \rightarrow x_1=3 \mbox{ and } x_2=5.\\
f''(x)=6x -24 \quad \rightarrow x_3 = 4.
\enn

We evaluate the three points we obtained. From the first derivatives, we found
two points that we evaluate in the second derivative: $f''(x_1)=f''(3)=-6$ (local maxima), and $f''(x_2)=f''(5)=6$ (local minima). Then, we evaluate the point
found in the second derivative at the third derivative: $f^{3)}(x_3)=f^{3)}(4)=6$, which means that we have an inflection point. Try now to plot the result!

\subsubsection{L'H{\^o}pital theorem}

This theorem will allow us to solve some limits that led to an indeterminate form. 
{\bf Theorem:} If $f$ and $g$ are two functions derivable in a neighborhood of $a\in \mathbb{R}$ (meaning that it works even if there is a hole) and if $\lim_{x  \rightarrow a} \frac{f(x)}{g(x)}=\frac{\infty}{\infty} \mbox{ or }\frac{0}{0}$ (and $a$ could be $\infty$):

\bnn
	\lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}=\lim_{x \rightarrow a} \frac{f(x)}{g(x)},
\enn

provided that $\lim_{x \rightarrow a} \frac{f'(x)}{g'(x)}$ exists.

{\bf Example:} To compute the following limit, we applied L'H{\^o}pital twice:

\bnn
 \lim_{x \rightarrow 1}\left(  \frac{x}{x-1}-\frac{1}{\log x}\right) =  \lim_{x \rightarrow 1} \frac{x\log x-x+1}{(x-1)\log x} = \\
\underbrace{=}_{\mbox{{\tiny L'H{\^o}pital}}} \lim_{x \rightarrow 1}\frac{\log x+1-1}{\log x +\frac{x-1}{x}}\underbrace{=}_{\mbox{{\tiny L'H{\^o}pital}}} \lim_{x \rightarrow 1}\frac{1/x}{1/x+\frac{x-x+1}{x^2}}=\frac{1}{2}.
\enn

...smooth!
  
\subsubsection{A myriad of theorems}

There are many theorems involving derivatives more or less evident, and whose application is more or less powerful as well. Let's see just one example, the Mean Value Theorem due to Lagrange. 

{\bf Theorem:} Given a function $f$ which is continuous in $[a,b]$ and differentiable in $(a,b)$, then $\exists x_0 \in (a,b)$ such that:
\bnn
	f'(x_0)=\frac{f(b)-f(a)}{b-a}=\tan \alpha.
\enn

Plot it and convince yourself that this is true!

\subsubsection{Taylor Series}\label{taylor}

A Taylor series is an approximation of a function $f(x)$ in the neighborhood of a given point in terms of its derivatives. This technique has been developed by Brook Taylor($1685-1731$) and first published in 1715.

In a first step we can approximate the function $f(x)$ in the neighborhood around $x_0$ by the tangent through $x_0$

\bnn f(x)=f(x_{0})+f'(x_{0}) \, (x-x_{0})+\mbox{error}
\enn

\begin{figure}[!h]

    \centerline{\epsfxsize=12cm  \epsfbox{matlab/fig48.eps}}

    \caption{Approximation of a curve at a point $x_0$ with the tangent at $x_0$} \label{fig58}

\end{figure} \vs \vs

\centerline{\bf Can we do better than this? Yes if higher order derivatives are considered!}

\bnn f(x)=f(x_{0})+f'(x_0)\,(x-x_0)+\frac{1}{2!}\,f''(x_0)(x-x_0)^2+\frac{1}{3!}\,f'''(x_{0})(x-x_{0})^{3}+... \enn

\bnn
    \mbox{\bf Taylor series:} \qquad f(x)=\left. \sum^{\infty}_{n=0}\frac{1}{n!}
    \,\frac{d^n f(x)}{dx^n}\right|_{x=x_0}\,(x-x_0)^n \qquad
    \mbox{with}\quad n!=1\cdot2\cdot3\cdot ... \;\;\;\mbox{n-factorial}
\enn

A function $f(x)$ may be approximated by truncating a Taylor Expansion
around $x_{0}$ at the m-th order.

$\Rightarrow$ Polynomial representations of function work well if the error approaches $0$ as the order $n$ increases.

{\bf Error estimate:}

\bnn
    f(x)=\left. \sum^n_{k=0}\frac{1}{k!}\,\frac{d^n f(x)}{dx^n}\right|_{x=x_0} \,(x-x_0)^k+\underbrace{R_n(x)}_{\mbox{error}}
\enn



Lagrange formulation of the error $R_n$ in a Taylor expansion that is truncated at order $n$

\bnn
    R_n(x)=\left. \frac{(x-x_0)^{n+1}}{(n+1)!}\,\frac{d^n f(x)}{dx^n}\right|_{x=\xi} \qquad
    \xi=x_0+\delta\,(x-x_0) \quad 0< \delta <1
\enn \vs



{\bf Examples:}



Approximate the  function $\sin x$ up to the 7-th order for $x$ around $x_0=0$ (Maclaurin series) using a Taylor expansion

\bnn
    \sin x\approx \underbrace{\sin 0}_{0}\; +\;\underbrace{\cos 0\cdot x}_{x}\;+\;\underbrace{-\sin
    0\frac{1}{2!}\;x^{2}}_{0}\;-\;\frac{1}{3!}\;x^{3}\;+\;\frac{1}{5!}\;x^{5}-\;\frac{1}{7!}\;x^{7} \qquad
    \mbox{symmetries!!}
\enn

\begin{figure}[!ht]

    \centerline{\epsfxsize=10cm  \epsfbox{matlab/fig49.eps}}

    \caption{Steps of a Taylor expansion of $\sin x$ around $x_0=0$}  \label{fig59}

\end{figure} \svs

The further away you move from the expansion point, the more significant the higher order terms!!

\vs {\bf Specific expansion of important functions:}

\bnn
\sin x=x-\;\frac{1}{3!}\;x^3\;+\;\frac{1}{5!}\;x^5-\;....\;\frac{(-1)^{n+1}}{(2\,n+1)!}\;x^{2\,n+1} \qquad \mbox{only odd terms}
\enn

\bnn
\cos x=1-\;\frac{1}{2!}\;x^2\;+\;\frac{1}{4!}\;x^4-\;....\;\frac{(-1)^n}{2\,n!}\;x^{2\,n} \qquad\qquad \mbox{only even terms}
\enn

\bnn e^{x}=1+\;\frac{1}{2!}\;x^2\;+\;\frac{1}{3!}\;x^3\;+\;....\;\frac{1}{n!}\;x^n \enn

\bnn  \mbox{ln}(1+x)=x-\;\frac{x^{2}}{2!}\;+\;\frac{x^{3}}{3!}\;+\;....\;\frac{(-1)^{n+1}}{n!}\;x^{n+1}  \enn


\vs {\bf Euler's Formula: {\boldmath $e^{i\theta}=\cos\theta+i\,\sin\theta$}}

\bnn e^{i\,\theta}=1+i\theta +\frac{1}{2!}\underbrace{(i\,\theta)^2}_{-\theta^2}+ \frac{1}{3!}\underbrace{(i\,\theta)^3}_{-i\,\theta^3}+\frac{1}{4!}\underbrace{(i\,\theta)^4}_{\theta^4}+
\frac{1}{5!}\underbrace{(i\theta)^5}_{i\,\theta^5}+...\qquad\mbox{Taylor expansion around $\theta=0$}
\enn

\bnn
  \Rightarrow \qquad e^{i\,\theta}= \underbrace{1-\frac{1}{2!}\,\theta^2+\frac{1}{4!}\,\theta^4+...}_{\cos\theta} \quad
    +i\,(\,\underbrace{\theta-\frac{1}{3!}\,\theta^3+\frac{1}{5!}\,\theta^5+...}_{\sin\theta}\,) \qquad\mbox{q.e.d.}
\enn



